{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b544368",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-11T04:53:13.182979Z",
     "start_time": "2023-01-11T04:53:12.051783Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "265fe9ca",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-11T04:53:13.198818Z",
     "start_time": "2023-01-11T04:53:13.185938Z"
    }
   },
   "outputs": [],
   "source": [
    "# col 생략 없이 출력\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4756f4f0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-11T04:53:13.435265Z",
     "start_time": "2023-01-11T04:53:13.200818Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 54879 entries, 0 to 54878\n",
      "Data columns (total 3 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   index   54879 non-null  int64 \n",
      " 1   text    54879 non-null  object\n",
      " 2   score   54879 non-null  int64 \n",
      "dtypes: int64(2), object(1)\n",
      "memory usage: 1.3+ MB\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"nlp_data/practice.csv\")\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f85616eb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-11T04:53:13.451363Z",
     "start_time": "2023-01-11T04:53:13.439269Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = df.drop('index',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "71f8669a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-11T04:53:13.482241Z",
     "start_time": "2023-01-11T04:53:13.454373Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>He was almost choking. There was so much, so m...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>“Your sister asked for it, I suppose?”</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>She was engaged one day as she walked, in per...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The captain was in the porch, keeping himself ...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>“Have mercy, gentlemen!” odin flung up his han...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  score\n",
       "0  He was almost choking. There was so much, so m...      3\n",
       "1             “Your sister asked for it, I suppose?”      2\n",
       "2   She was engaged one day as she walked, in per...      1\n",
       "3  The captain was in the porch, keeping himself ...      4\n",
       "4  “Have mercy, gentlemen!” odin flung up his han...      3"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09726f8d",
   "metadata": {},
   "source": [
    "> **Problem 0**   \n",
    "> score는 사용...?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a750ccc",
   "metadata": {},
   "source": [
    "# 1. temp 변수로 연습"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b063b1a6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-09T07:47:55.850812Z",
     "start_time": "2023-01-09T07:47:55.839185Z"
    }
   },
   "source": [
    "## 기본적인 토큰화 연습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c705ddb6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-11T04:53:13.498245Z",
     "start_time": "2023-01-11T04:53:13.484202Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'He was almost choking. There was so much, so much he wanted to say, but strange exclamations were all that came from his lips. The Pole gazed fixedly at him, at the bundle of notes in his hand; looked at odin, and was in evident perplexity.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = df.iloc[0,0]\n",
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b4ac6173",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-11T04:53:15.756535Z",
     "start_time": "2023-01-11T04:53:13.500201Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문장 토큰화 :  ['He was almost choking.', 'There was so much, so much he wanted to say, but strange exclamations were all that came from his lips.', 'The Pole gazed fixedly at him, at the bundle of notes in his hand; looked at odin, and was in evident perplexity.']\n"
     ]
    }
   ],
   "source": [
    "# 문장 토큰화 시도\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "print('문장 토큰화 : ', sent_tokenize(temp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cc6ab379",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-11T04:53:22.346764Z",
     "start_time": "2023-01-11T04:53:15.757536Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 토큰화 : ['he', 'was', 'almost', 'choking', 'there', 'was', 'so', 'much', 'so', 'much', 'he', 'wanted', 'to', 'say', 'but', 'strange', 'exclamations', 'were', 'all', 'that', 'came', 'from', 'his', 'lips', 'the', 'pole', 'gazed', 'fixedly', 'at', 'him', 'at', 'the', 'bundle', 'of', 'notes', 'in', 'his', 'hand', 'looked', 'at', 'odin', 'and', 'was', 'in', 'evident', 'perplexity']\n"
     ]
    }
   ],
   "source": [
    "# 단어 토큰화\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
    "\n",
    "# text_to_word_sequence 사용 - 모두 소문자로 통일 + 구두점 제거해주기 때문에 선택\n",
    "print('단어 토큰화 :',text_to_word_sequence(temp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ca39eb54",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-11T04:53:22.362864Z",
     "start_time": "2023-01-11T04:53:22.349758Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "트리뱅크 워드토크나이저 :  ['He', 'was', 'almost', 'choking.', 'There', 'was', 'so', 'much', ',', 'so', 'much', 'he', 'wanted', 'to', 'say', ',', 'but', 'strange', 'exclamations', 'were', 'all', 'that', 'came', 'from', 'his', 'lips.', 'The', 'Pole', 'gazed', 'fixedly', 'at', 'him', ',', 'at', 'the', 'bundle', 'of', 'notes', 'in', 'his', 'hand', ';', 'looked', 'at', 'odin', ',', 'and', 'was', 'in', 'evident', 'perplexity', '.']\n"
     ]
    }
   ],
   "source": [
    "# 표준 토큰화 예제 : Penn Treebank Tokenization\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "print('트리뱅크 워드토크나이저 : ', tokenizer.tokenize(temp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "83d636d2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-11T04:53:22.473092Z",
     "start_time": "2023-01-11T04:53:22.366777Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 토큰화 : ['he', 'was', 'almost', 'choking', 'there', 'was', 'so', 'much', 'so', 'much', 'he', 'wanted', 'to', 'say', 'but', 'strange', 'exclamations', 'were', 'all', 'that', 'came', 'from', 'his', 'lips', 'the', 'pole', 'gazed', 'fixedly', 'at', 'him', 'at', 'the', 'bundle', 'of', 'notes', 'in', 'his', 'hand', 'looked', 'at', 'odin', 'and', 'was', 'in', 'evident', 'perplexity']\n",
      "품사 태깅 : [('he', 'PRP'), ('was', 'VBD'), ('almost', 'RB'), ('choking', 'VBG'), ('there', 'EX'), ('was', 'VBD'), ('so', 'RB'), ('much', 'RB'), ('so', 'RB'), ('much', 'JJ'), ('he', 'PRP'), ('wanted', 'VBD'), ('to', 'TO'), ('say', 'VB'), ('but', 'CC'), ('strange', 'JJ'), ('exclamations', 'NNS'), ('were', 'VBD'), ('all', 'PDT'), ('that', 'WDT'), ('came', 'VBD'), ('from', 'IN'), ('his', 'PRP$'), ('lips', 'NNS'), ('the', 'DT'), ('pole', 'NN'), ('gazed', 'VBD'), ('fixedly', 'RB'), ('at', 'IN'), ('him', 'PRP'), ('at', 'IN'), ('the', 'DT'), ('bundle', 'NN'), ('of', 'IN'), ('notes', 'NNS'), ('in', 'IN'), ('his', 'PRP$'), ('hand', 'NN'), ('looked', 'VBD'), ('at', 'IN'), ('odin', 'NN'), ('and', 'CC'), ('was', 'VBD'), ('in', 'IN'), ('evident', 'JJ'), ('perplexity', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "# 품사 태깅\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "\n",
    "tokenized_sentence = text_to_word_sequence(temp)  # text_to_word_sequence 사용\n",
    "\n",
    "print('단어 토큰화 :',tokenized_sentence)\n",
    "print('품사 태깅 :',pos_tag(tokenized_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "501813c9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-11T05:08:39.945473Z",
     "start_time": "2023-01-11T05:08:39.930354Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'J': 'a', 'N': 'n', 'V': 'v', 'R': 'r'}\n",
      "n\n"
     ]
    }
   ],
   "source": [
    "tag_dict = {\"J\": wordnet.ADJ, \n",
    "            \"N\": wordnet.NOUN,\n",
    "            \"V\": wordnet.VERB,\n",
    "            \"R\": wordnet.ADV}\n",
    "print(tag_dict)\n",
    "print(tag_dict.get('D', wordnet.NOUN))   \n",
    "# 첫번째 인자가 tag_dict에 있다면 해당 인자(key)에 대한 value를, 없다면 wordnet.NOUN을 출력"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae57471d",
   "metadata": {},
   "source": [
    "## 정규화 및 클리닝"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c71fd48b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-11T05:38:21.760251Z",
     "start_time": "2023-01-11T05:38:21.749571Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "표제어 추출 전 : ['he', 'was', 'almost', 'choking', 'there', 'was', 'so', 'much', 'so', 'much', 'he', 'wanted', 'to', 'say', 'but', 'strange', 'exclamations', 'were', 'all', 'that', 'came', 'from', 'his', 'lips', 'the', 'pole', 'gazed', 'fixedly', 'at', 'him', 'at', 'the', 'bundle', 'of', 'notes', 'in', 'his', 'hand', 'looked', 'at', 'odin', 'and', 'was', 'in', 'evident', 'perplexity']\n",
      "표제어 추출 후 : ['he', 'wa', 'almost', 'choking', 'there', 'wa', 'so', 'much', 'so', 'much', 'he', 'wanted', 'to', 'say', 'but', 'strange', 'exclamation', 'were', 'all', 'that', 'came', 'from', 'his', 'lip', 'the', 'pole', 'gazed', 'fixedly', 'at', 'him', 'at', 'the', 'bundle', 'of', 'note', 'in', 'his', 'hand', 'looked', 'at', 'odin', 'and', 'wa', 'in', 'evident', 'perplexity']\n"
     ]
    }
   ],
   "source": [
    "# 정규화 - 표제어 추출\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "words = tokenized_sentence\n",
    "\n",
    "print('표제어 추출 전 :',words)\n",
    "print('표제어 추출 후 :',[lemmatizer.lemmatize(word) for word in words])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d6710b",
   "metadata": {},
   "source": [
    "> **Problem 1**         \n",
    "> was가 wa로 나오는데 was(be)로 만드는 방법 모르겠음..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "12198fe4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-11T05:40:10.773640Z",
     "start_time": "2023-01-11T05:40:10.466361Z"
    }
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [67]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mlemmatizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlemmatize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mwas\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mam\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mn\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\data\\lib\\site-packages\\nltk\\stem\\wordnet.py:45\u001b[0m, in \u001b[0;36mWordNetLemmatizer.lemmatize\u001b[1;34m(self, word, pos)\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlemmatize\u001b[39m(\u001b[38;5;28mself\u001b[39m, word: \u001b[38;5;28mstr\u001b[39m, pos: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[0;32m     34\u001b[0m     \u001b[38;5;124;03m\"\"\"Lemmatize `word` using WordNet's built-in morphy function.\u001b[39;00m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;124;03m    Returns the input word unchanged if it cannot be found in WordNet.\u001b[39;00m\n\u001b[0;32m     36\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;124;03m    :return: The lemma of `word`, for the given `pos`.\u001b[39;00m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 45\u001b[0m     lemmas \u001b[38;5;241m=\u001b[39m \u001b[43mwn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_morphy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mword\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpos\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     46\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mmin\u001b[39m(lemmas, key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m lemmas \u001b[38;5;28;01melse\u001b[39;00m word\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\data\\lib\\site-packages\\nltk\\corpus\\reader\\wordnet.py:2072\u001b[0m, in \u001b[0;36mWordNetCorpusReader._morphy\u001b[1;34m(self, form, pos, check_exceptions)\u001b[0m\n\u001b[0;32m   2064\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_morphy\u001b[39m(\u001b[38;5;28mself\u001b[39m, form, pos, check_exceptions\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m   2065\u001b[0m     \u001b[38;5;66;03m# from jordanbg:\u001b[39;00m\n\u001b[0;32m   2066\u001b[0m     \u001b[38;5;66;03m# Given an original string x\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2069\u001b[0m     \u001b[38;5;66;03m# 3. If there are no matches, keep applying rules until you either\u001b[39;00m\n\u001b[0;32m   2070\u001b[0m     \u001b[38;5;66;03m#    find a match or you can't go any further\u001b[39;00m\n\u001b[1;32m-> 2072\u001b[0m     exceptions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_exception_map\u001b[49m\u001b[43m[\u001b[49m\u001b[43mpos\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m   2073\u001b[0m     substitutions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mMORPHOLOGICAL_SUBSTITUTIONS[pos]\n\u001b[0;32m   2075\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_rules\u001b[39m(forms):\n",
      "\u001b[1;31mTypeError\u001b[0m: unhashable type: 'list'"
     ]
    }
   ],
   "source": [
    "print(lemmatizer.lemmatize(['was','am'], ['v','n']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "53596702",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-11T05:38:22.272094Z",
     "start_time": "2023-01-11T05:38:22.249020Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "어간 추출 전 : ['he', 'was', 'almost', 'choking', 'there', 'was', 'so', 'much', 'so', 'much', 'he', 'wanted', 'to', 'say', 'but', 'strange', 'exclamations', 'were', 'all', 'that', 'came', 'from', 'his', 'lips', 'the', 'pole', 'gazed', 'fixedly', 'at', 'him', 'at', 'the', 'bundle', 'of', 'notes', 'in', 'his', 'hand', 'looked', 'at', 'odin', 'and', 'was', 'in', 'evident', 'perplexity']\n",
      "포터 스테머의 어간 추출 후: ['he', 'wa', 'almost', 'choke', 'there', 'wa', 'so', 'much', 'so', 'much', 'he', 'want', 'to', 'say', 'but', 'strang', 'exclam', 'were', 'all', 'that', 'came', 'from', 'hi', 'lip', 'the', 'pole', 'gaze', 'fixedli', 'at', 'him', 'at', 'the', 'bundl', 'of', 'note', 'in', 'hi', 'hand', 'look', 'at', 'odin', 'and', 'wa', 'in', 'evid', 'perplex']\n"
     ]
    }
   ],
   "source": [
    "# 정규화 - 어간 추출\n",
    "from nltk.stem import PorterStemmer   # 포터 알고리즘\n",
    "\n",
    "porter_stemmer = PorterStemmer()\n",
    "words= tokenized_sentence\n",
    "\n",
    "print('어간 추출 전 :', words)\n",
    "print('포터 스테머의 어간 추출 후:',[porter_stemmer.stem(w) for w in words])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac409884",
   "metadata": {},
   "source": [
    "> strang, exclam, hi, fixedli 등 이상하게 나온 애들 많음... 어간 추출은 별로 좋지 않은 방법인 듯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "35eb1f5e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-11T05:38:22.676783Z",
     "start_time": "2023-01-11T05:38:22.554376Z"
    }
   },
   "outputs": [],
   "source": [
    "# 클리닝 - 불용어 제거\n",
    "from nltk.corpus import stopwords\n",
    "from konlpy.tag import Okt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "0de1a14e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-11T05:38:22.785299Z",
     "start_time": "2023-01-11T05:38:22.765296Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "불용어 개수 : 179\n",
      "불용어 10개 출력 : ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]\n"
     ]
    }
   ],
   "source": [
    "# NLTK에서 불용어 확인하기\n",
    "# stopwords.words(\"english\") : NLTK가 정의한 영어 불용어 리스트를 리턴\n",
    "stop_words_list = stopwords.words('english')\n",
    "print('불용어 개수 :', len(stop_words_list))\n",
    "print('불용어 10개 출력 :',stop_words_list[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "0d72041a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-11T05:38:23.235910Z",
     "start_time": "2023-01-11T05:38:23.217784Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "불용어 제거 전 : ['he', 'was', 'almost', 'choking', 'there', 'was', 'so', 'much', 'so', 'much', 'he', 'wanted', 'to', 'say', 'but', 'strange', 'exclamations', 'were', 'all', 'that', 'came', 'from', 'his', 'lips', 'the', 'pole', 'gazed', 'fixedly', 'at', 'him', 'at', 'the', 'bundle', 'of', 'notes', 'in', 'his', 'hand', 'looked', 'at', 'odin', 'and', 'was', 'in', 'evident', 'perplexity']\n",
      "불용어 제거 후 : ['almost', 'choking', 'much', 'much', 'wanted', 'say', 'strange', 'exclamations', 'came', 'lips', 'pole', 'gazed', 'fixedly', 'bundle', 'notes', 'hand', 'looked', 'odin', 'evident', 'perplexity']\n"
     ]
    }
   ],
   "source": [
    "# NLTK를 통해서 불용어 제거하기\n",
    "example = temp\n",
    "stop_words = set(stopwords.words('english')) \n",
    "\n",
    "word_tokens = text_to_word_sequence(example)   # text_to_word_sequence 사용\n",
    "\n",
    "result = []\n",
    "for word in word_tokens: \n",
    "    if word not in stop_words: \n",
    "        result.append(word) \n",
    "\n",
    "print('불용어 제거 전 :',word_tokens) \n",
    "print('불용어 제거 후 :',result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "2196d75d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-11T05:38:23.731349Z",
     "start_time": "2023-01-11T05:38:23.719761Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "표제어 추출 전 : ['almost', 'choking', 'much', 'much', 'wanted', 'say', 'strange', 'exclamations', 'came', 'lips', 'pole', 'gazed', 'fixedly', 'bundle', 'notes', 'hand', 'looked', 'odin', 'evident', 'perplexity']\n",
      "표제어 추출 후 : ['almost', 'choking', 'much', 'much', 'wanted', 'say', 'strange', 'exclamation', 'came', 'lip', 'pole', 'gazed', 'fixedly', 'bundle', 'note', 'hand', 'looked', 'odin', 'evident', 'perplexity']\n"
     ]
    }
   ],
   "source": [
    "# 불용어 제거 후 표제어 추출까지 해보기!!\n",
    "print('표제어 추출 전 :',result)\n",
    "print('표제어 추출 후 :',[lemmatizer.lemmatize(word) for word in result])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "322500d9",
   "metadata": {},
   "source": [
    "> **Problem 2**   \n",
    "> 표제어 추출을 했는데,,, 과거형이 그대로임... 이거 'v'로 품사정보를 알려줘야 되는 것 같은데 어떻게 알려주지...?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "4a38341f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-11T05:38:24.650246Z",
     "start_time": "2023-01-11T05:38:24.629034Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "어간 추출 전 : ['almost', 'choking', 'much', 'much', 'wanted', 'say', 'strange', 'exclamations', 'came', 'lips', 'pole', 'gazed', 'fixedly', 'bundle', 'notes', 'hand', 'looked', 'odin', 'evident', 'perplexity']\n",
      "포터 스테머의 어간 추출 후: ['almost', 'choke', 'much', 'much', 'want', 'say', 'strang', 'exclam', 'came', 'lip', 'pole', 'gaze', 'fixedli', 'bundl', 'note', 'hand', 'look', 'odin', 'evid', 'perplex']\n"
     ]
    }
   ],
   "source": [
    "# 불용어 제거 후 어간 추출까지 해보기!!\n",
    "print('어간 추출 전 :', result)\n",
    "print('포터 스테머의 어간 추출 후:',[porter_stemmer.stem(w) for w in result])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b6083c",
   "metadata": {},
   "source": [
    "> **Problem 3**   \n",
    "> 얘는 과거형 문제는 해결되는데 e로 끝나는 명사의 경우 e가 사라져서 단어 형태가 이상해짐. - strang, exlam, fixedli, bundl 등...   \n",
    "> 과거형도 came의 경우 안바뀜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "0d27b04b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-11T05:38:28.521648Z",
     "start_time": "2023-01-11T05:38:28.482704Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['VB', 'CC', 'NN', 'PR', 'TO', 'RB', 'JJ', 'IN', 'DT']"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 품사 고유값 확인\n",
    "a = [pos_tag([w])[0][1][:2] for w in tokenized_sentence]\n",
    "b = list(set(a))\n",
    "b   # 품사 고유값 - 9개"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "042b8c8c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-11T05:39:05.587467Z",
     "start_time": "2023-01-11T05:39:05.551181Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "be\n",
      "['almost', 'choke', 'much', 'much', 'want', 'say', 'strange', 'exclamation', 'come', 'lip', 'pole', 'gaze', 'fixedly', 'bundle', 'note', 'hand', 'look', 'odin', 'evident', 'perplexity']\n"
     ]
    }
   ],
   "source": [
    "# Lemmatize with POS Tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "def get_wordnet_pos(word):\n",
    "   #특정 단어의 품사(ex. VBD)의 앞글자(V)를 대문자로 추출\n",
    "    tag = pos_tag([word])[0][1][:2].upper() \n",
    "\n",
    "   #품사 딕셔너리 생성\n",
    "    tag_dict = {\"VB\" : wordnet.VERB,\n",
    "                #\"CC\" : wordnet.CONJ,\n",
    "                \"NN\" : wordnet.NOUN,\n",
    "                #\"PR\" : wordnet.PRON,\n",
    "                #\"TO\" : wordnet.X,\n",
    "                \"RB\" : wordnet.ADV,\n",
    "                \"JJ\" : wordnet.ADJ,\n",
    "                #\"IN\" : wordnet.X,\n",
    "                #\"DT\" : wordnet.DET\n",
    "               }\n",
    "    \n",
    "   # tag라는 키가 tag_dict에 있다면 해당 키에 대한 value, 없다면 wordnet.NOUN 출력\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "# 단어 표제어 추출\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "word = 'was'\n",
    "print(lemmatizer.lemmatize(word, get_wordnet_pos(word)))\n",
    "\n",
    "# 문장 표제어 추출\n",
    "print([lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in result])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f2cb5a2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-11T04:53:22.958536Z",
     "start_time": "2023-01-11T04:53:22.958536Z"
    }
   },
   "outputs": [],
   "source": [
    "# 클리닝 - 정규표현식\n",
    "# 규칙이 딱히 안보임. (크롤링한 데이터가 아니기 때문) -> 필요없음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f52128",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-09T07:08:13.704727Z",
     "start_time": "2023-01-09T07:08:13.686683Z"
    }
   },
   "source": [
    "## 정수 인코딩 - 맵핑!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdde6100",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-11T04:53:22.960536Z",
     "start_time": "2023-01-11T04:53:22.960536Z"
    }
   },
   "outputs": [],
   "source": [
    "# 1. 문장 토큰화\n",
    "sentences = sent_tokenize(temp)\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d98174",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-11T04:53:22.963536Z",
     "start_time": "2023-01-11T04:53:22.963536Z"
    }
   },
   "outputs": [],
   "source": [
    "# 2. 1을 바탕으로 단어 토큰화 수행 : 정제 + 정규화 작업 병행\n",
    "vocab = {}\n",
    "preprocessed_sentences = []\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# 단어 토큰화(소문자화+구두점 제거) -> 불용어 제거 -> 단어길이 2이하인 경우 제거\n",
    "for sentence in sentences:\n",
    "    # 단어 토큰화\n",
    "    tokenized_sentence = text_to_word_sequence(sentence)\n",
    "    result = []\n",
    "\n",
    "    for word in tokenized_sentence: \n",
    "        if word not in stop_words: # 단어 토큰화 된 결과에 대해 불용어 제거\n",
    "            if len(word) > 2: # 단어 길이가 2이하인 경우 제거\n",
    "                result.append(word)\n",
    "                if word not in vocab:\n",
    "                    vocab[word] = 0 \n",
    "                vocab[word] += 1\n",
    "    preprocessed_sentences.append(result) \n",
    "print(preprocessed_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ec17a4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-11T04:53:22.965534Z",
     "start_time": "2023-01-11T04:53:22.965534Z"
    }
   },
   "outputs": [],
   "source": [
    "# 3. 인덱싱 - nltk의 FreqDist 사용하기\n",
    "from nltk import FreqDist\n",
    "import numpy as np\n",
    "\n",
    "# np.hstack으로 문장 구분을 제거 - array 형태로 반환\n",
    "vocab = FreqDist(np.hstack(preprocessed_sentences))\n",
    "\n",
    "# most_common(n)는 상위 빈도수를 가진 n개의 단어만을 리턴\n",
    "vocab_size = 5\n",
    "vocab = vocab.most_common(vocab_size) # 등장 빈도수가 높은 상위 5개의 단어만 저장\n",
    "print(vocab)\n",
    "\n",
    "# 높은 빈도수를 가진 단어일수록 낮은 정수 인덱스 부여\n",
    "word_to_index = {word[0] : index + 1 for index, word in enumerate(vocab)}\n",
    "print(word_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b010884",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-11T04:53:22.967535Z",
     "start_time": "2023-01-11T04:53:22.967535Z"
    }
   },
   "outputs": [],
   "source": [
    "# 4. 매핑\n",
    "word_to_index['OOV'] = len(word_to_index) + 1\n",
    "print(word_to_index)\n",
    "\n",
    "# 매핑(mapping)\n",
    "encoded_sentences = []\n",
    "for sentence in preprocessed_sentences:\n",
    "    encoded_sentence = []\n",
    "    for word in sentence:\n",
    "        try:\n",
    "            # 단어 집합에 있는 단어라면 해당 단어의 정수를 리턴.\n",
    "            encoded_sentence.append(word_to_index[word])\n",
    "        except KeyError:\n",
    "            # 만약 단어 집합에 없는 단어라면 'OOV'의 정수를 리턴.\n",
    "            encoded_sentence.append(word_to_index['OOV'])\n",
    "    encoded_sentences.append(encoded_sentence)\n",
    "print(encoded_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12f621b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-11T04:53:22.969534Z",
     "start_time": "2023-01-11T04:53:22.969534Z"
    }
   },
   "outputs": [],
   "source": [
    "# 5-1. 케라스 토크나이저 이용하여 매핑해보기\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "tokenizer = Tokenizer()\n",
    "\n",
    "# fit_on_texts()안에 코퍼스를 입력으로 하면 빈도수를 기준으로 단어 집합을 생성.\n",
    "# 입력한 텍스트로부터 단어 빈도수가 높은 순으로 낮은 정수 인덱스를 부여\n",
    "# 빈도수 높은 상위 5개의 단어만 사용하겠다고 지정\n",
    "vocab_size = 5\n",
    "tokenizer = Tokenizer(num_words = vocab_size + 1) # 상위 5개 단어만 사용\n",
    "tokenizer.fit_on_texts(preprocessed_sentences)\n",
    "\n",
    "# 각 단어에 인덱스가 어떻게 부여되었는지를 보려면 word_index를 사용한다!\n",
    "print(tokenizer.word_index)   # 변화X\n",
    "\n",
    "# 각 단어가 카운트를 수행하였을 때 몇 개였는지를 보고자 한다면 word_counts를 사용한다!\n",
    "print(tokenizer.word_counts)  # 변화X\n",
    "\n",
    "# texts_to_sequences()는 입력으로 들어온 코퍼스에 대해서 각 단어를 이미 정해진 인덱스로 변환한다.\n",
    "print(tokenizer.texts_to_sequences(preprocessed_sentences))  # 여기서만 적용됨\n",
    "   # 빈도수가 높은 5개 단어에 1,2,3,4,5가 부여되었고, 나머지 단어들은 인덱스 변환이 실행되지 않고 제거됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f97a287",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-11T04:53:22.972535Z",
     "start_time": "2023-01-11T04:53:22.972535Z"
    }
   },
   "outputs": [],
   "source": [
    "# 5-2. 단어 집합에 없는 단어들은 OOV로 간주하여 보존하고 싶다면 Tokenizer의 인자 oov_token을 사용\n",
    "# 숫자 0과 OOV를 고려해서 단어 집합의 크기는 +2\n",
    "vocab_size = 5\n",
    "tokenizer = Tokenizer(num_words = vocab_size + 2, oov_token = 'OOV')\n",
    "tokenizer.fit_on_texts(preprocessed_sentences)\n",
    "\n",
    "print('단어 OOV의 인덱스 : {}'.format(tokenizer.word_index['OOV']))\n",
    "\n",
    "# 코퍼스에 대해 정수 인코딩 진행\n",
    "print(tokenizer.texts_to_sequences(preprocessed_sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e77b6dc9",
   "metadata": {},
   "source": [
    "> 빈도수 상위 5개의 단어는 2 ~ 6까지의 인덱스를 가졌으며, 그 외 단어 집합에 없는 'good'과 같은 단어들은 전부 'OOV'의 인덱스인 1로 인코딩됨."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c7d1dbf",
   "metadata": {},
   "source": [
    "## 패딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e731790f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-11T04:53:22.975554Z",
     "start_time": "2023-01-11T04:53:22.975554Z"
    }
   },
   "outputs": [],
   "source": [
    "# 케라스 전처리 도구로 패딩하기\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "encoded = tokenizer.texts_to_sequences(preprocessed_sentences)\n",
    "print(encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d881066",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-11T04:53:22.977049Z",
     "start_time": "2023-01-11T04:53:22.977049Z"
    }
   },
   "outputs": [],
   "source": [
    "# maxlen의 인자로 정수를 주면, 해당 정수로 모든 문서의 길이를 동일하게 함.\n",
    "padded = pad_sequences(encoded, padding='post', maxlen=5)   # 뒤를 0으로 채우기\n",
    "padded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d9720c9",
   "metadata": {},
   "source": [
    "## 데이터의 분리 - ???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b588f85c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-11T04:53:22.979062Z",
     "start_time": "2023-01-11T04:53:22.979062Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35333ab7",
   "metadata": {},
   "source": [
    "# 2. 전체 데이터로 해보기 - 맞는지...?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f85b961",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-11T04:53:22.981466Z",
     "start_time": "2023-01-11T04:53:22.981466Z"
    }
   },
   "outputs": [],
   "source": [
    "data = []\n",
    "\n",
    "for temp in df['text']:\n",
    "    # 1. 문장 토큰화\n",
    "    sentences = sent_tokenize(temp)\n",
    "    #print(sentences)\n",
    "    \n",
    "    # 2. 1을 바탕으로 단어 토큰화 수행 : 정제 + 정규화 작업 병행\n",
    "    vocab = {}\n",
    "    preprocessed_sentences = []\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    # 단어 토큰화(소문자화+구두점 제거) -> 불용어 제거 -> 단어길이 2이하인 경우 제거\n",
    "    for sentence in sentences:\n",
    "        # 단어 토큰화\n",
    "        tokenized_sentence = text_to_word_sequence(sentence)\n",
    "        result = []\n",
    "    \n",
    "        for word in tokenized_sentence: \n",
    "            if word not in stop_words: # 단어 토큰화 된 결과에 대해 불용어 제거\n",
    "                if len(word) > 2: # 단어 길이가 2이하인 경우 제거\n",
    "                    result.append(word)\n",
    "                    if word not in vocab:\n",
    "                        vocab[word] = 0 \n",
    "                    vocab[word] += 1\n",
    "        preprocessed_sentences.append(result) \n",
    "    #print(preprocessed_sentences)\n",
    "    \n",
    "    # 케라스 토크나이저 이용하여 매핑\n",
    "    tokenizer = Tokenizer()\n",
    "\n",
    "    # fit_on_texts()안에 코퍼스를 입력으로 하면 빈도수를 기준으로 단어 집합을 생성.\n",
    "    # 입력한 텍스트로부터 단어 빈도수가 높은 순으로 낮은 정수 인덱스를 부여\n",
    "    # 빈도수 높은 상위 5개의 단어만 사용하겠다고 지정\n",
    "    # 단어 집합에 없는 단어들은 OOV로 간주하여 보존하고 싶다면 Tokenizer의 인자 oov_token을 사용\n",
    "    vocab_size = 5\n",
    "    tokenizer = Tokenizer(num_words = vocab_size + 2, oov_token = 'OOV')\n",
    "    tokenizer.fit_on_texts(preprocessed_sentences)\n",
    "\n",
    "    #print('단어 OOV의 인덱스 : {}'.format(tokenizer.word_index['OOV']))\n",
    "\n",
    "    # 코퍼스에 대해 정수 인코딩 진행\n",
    "    encoded = tokenizer.texts_to_sequences(preprocessed_sentences)\n",
    "    #print(encoded)   # 모든 단어가 고유한 정수로 변환됨\n",
    "    \n",
    "    # maxlen의 인자로 정수를 주면, 해당 정수로 모든 문서의 길이를 동일하게 함.\n",
    "    padded = pad_sequences(encoded, padding='post', maxlen=5)   # 뒤를 0으로 채우기\n",
    "    #print(padded)\n",
    "    data.append(padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7735885",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-11T04:53:22.984478Z",
     "start_time": "2023-01-11T04:53:22.984478Z"
    }
   },
   "outputs": [],
   "source": [
    "print(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f7be97f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-11T04:53:22.989476Z",
     "start_time": "2023-01-11T04:53:22.989476Z"
    }
   },
   "outputs": [],
   "source": [
    "data[:10]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
