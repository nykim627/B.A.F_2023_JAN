{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1f200f0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-16T11:36:22.421149Z",
     "start_time": "2023-01-16T11:36:22.398100Z"
    }
   },
   "source": [
    "**_♡텍스트 마이닝 사용 설명서♡ : (1) ~ (4)편_**\n",
    "\n",
    "**(1) 텍스트 분석 기초**   \n",
    "https://dacon.io/competitions/official/235946/codeshare/5986?page=1&dtype=random   \n",
    "**(2) 감성 분석**   \n",
    "https://dacon.io/competitions/official/235946/codeshare/6006?page=1&dtype=random   \n",
    "**(3) 토픽 모델링**   \n",
    "https://dacon.io/competitions/official/235946/codeshare/6017?page=1&dtype=random   \n",
    "**(4) 문서 군집화와 문서 유사도**    \n",
    "https://dacon.io/competitions/official/235946/codeshare/6031?page=1&dtype=random\n",
    "   \n",
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb4c3f3",
   "metadata": {},
   "source": [
    "# 1. 토픽 모델링(Topic Modeling)이란?\n",
    "다수의 문서 집합 안에서 같이 사용되는 단어의 집합으로 문서에 내재되어 있는 토픽을 분석하는 기법\n",
    "- 주로 연구 동향이나 사회적 현상을 파악하기 위한 연구 기법으로 활용   \n",
    "- 워드클라우드처럼 단순히 단어의 빈도 수를 세는 것에서 넘어 함께 사용되는 단어를 묶어줌으로써 문서의 내용을 구체적으로 예상할 수 있다.\n",
    "- LDA(Latent Dirichlet Allocation) 알고리즘과 LSA(Latent Semantic Analysis) 알고리즘"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeac4f92",
   "metadata": {},
   "source": [
    "# 2. LDA(Latent Dirichlet Allocation)\n",
    "토픽 모델링에서 가장 빈번하게 사용.   \n",
    "\n",
    "함께 자주 나타나는 단어의 **토픽**을 찾는 것을 목표로 하는 모델.\n",
    "- 머신러닝에서의 토픽 : 의미가 있든 없든 PCA나 NMF로 추출한 성분에 가까운 것. 일반적인 주제와는 관련 X\n",
    "\n",
    "사이킷런은 LDA 기반의 토픽 모델링을 LatentDirichletAllocation 클래스로 제공한다.   \n",
    "또한, LDA는 Count 기반의 벡터화만 사용한다.\n",
    "\n",
    "**[LatentDirichletAllocation의 주요 파라미터]**\n",
    "1) **c_components** : 토픽의 개수 지정   \n",
    "2) **learning_method** : 'batch' 또는 'online'으로 설정. 디폴트는 online. 데이터 크기가 크면 일반적으로 batch로 설정. online이 batch보다 업데이트 속도는 빠름.\n",
    "\n",
    "**[CountVectorizer의 주요 파라미터]**\n",
    "1) **max_df** : 전체 문서에 걸쳐서 너무 높은 빈도수를 가지는 단어 피처를 제외하기 위한 파라미터   \n",
    "2) **min_df** : 전체 문서에 걸쳐서 너무 낮은 빈도수를 가지는 단어 피처를 제외하기 위한 파라미터   \n",
    "3) **max_features** : 추출하는 피처의 개수를 제한하며 정수로 값을 지정   \n",
    "4) **stop_words** : \"english\"로 지정하면 영어의 스롭워드로 지정된 단어는 추출에서 제외   \n",
    "5) **n_gram_range** : BOW 모델의 단어 순서를 어느 정도 보강하기 위한 n_gram 범위 설정. 튜플 형태로 (범위 최솟값, 범위 최댓값)을 지정.   \n",
    "6) **analyzer** : 피처 추출을 수행한 단위를 지정. 디폴트는 \"word\"   \n",
    "7) **token_pattern** : 토큰화를 수행하는 정규 표현식 패터을 지정. 디폴트는 공백 또는 개행 문자 등으로 구분된 단어 분리자 사이의 2문자 이상 단어.   \n",
    "8) **tokenizer** : 토큰화를 변도의 커스텀 함수로 이용 시 적용.   \n",
    "\n",
    "**[LDA에 활용되는 주요 함수]**\n",
    "1) **LatentDirichletAllocation.fit(df)** : LatentDirichletAllocation 객체는 components_속성값을 갖게 됨.   \n",
    "2) **.components_** : 개별 토픽별로 각 word 피처가 얼마나 많이 그 토픽에 할당됐는지에 대한 수치   \n",
    "3) **count_vect.get_feature_names()** : 카운트 벡터화에 사용된 feature 단어 목록을 볼 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "530d4d9f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
